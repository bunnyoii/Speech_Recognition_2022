{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Import Packages and Load Audio\n",
    "\n",
    "In this step, we import the required libraries, such as `librosa` for audio processing, `numpy` for numerical computations, and `matplotlib` for visualization. We load the audio file and inspect its waveform to gain an initial understanding of the speech signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import librosa\n",
    "\n",
    "# Load the audio signal and sampling rate from the file\n",
    "signal, fs = librosa.load('record.wav', sr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pre-emphasis\n",
    "\n",
    "Pre-emphasis is applied to the signal to amplify higher frequencies, which are typically weaker in speech. This step helps to balance the frequency spectrum, improving the signal-to-noise ratio for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def pre_emphasis(signal, alpha=0.98):\n",
    "    \"\"\"\n",
    "    Apply pre-emphasis to the input audio signal.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): The input audio signal.\n",
    "        alpha (float): Pre-emphasis filter coefficient. Default is 0.98.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The emphasized signal.\n",
    "    \"\"\"\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - alpha * signal[:-1])\n",
    "    return emphasized_signal\n",
    "\n",
    "def normalize_signal(signal):\n",
    "    \"\"\"\n",
    "    Normalize the signal to have values between -1 and 1.\n",
    "    \n",
    "    Args:\n",
    "        signal (numpy.ndarray): The input signal.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The normalized signal.\n",
    "    \"\"\"\n",
    "    return signal / np.max(np.abs(signal))\n",
    "\n",
    "# Apply pre-emphasis to the original signal\n",
    "emphasized_signal = pre_emphasis(signal, alpha=0.98)  # 尝试增加 alpha 为 0.98\n",
    "emphasized_signal = normalize_signal(emphasized_signal)  # 归一化信号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Windowing\n",
    "\n",
    "In this step, the speech signal is divided into short overlapping frames by applying a window function (e.g., Hamming window). This allows for localized analysis of the signal in both time and frequency domains, necessary for short-term spectral processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framing(signal, frame_size, frame_stride, fs):\n",
    "    \"\"\"\n",
    "    Frame the signal into overlapping frames and apply a window function (Hamming window).\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): The input audio signal.\n",
    "        frame_size (float): Frame size in seconds.\n",
    "        frame_stride (float): Frame stride in seconds.\n",
    "        fs (int): Sampling rate of the signal.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array where each row is a frame of the signal.\n",
    "    \"\"\"\n",
    "    # Convert frame size and stride from seconds to samples\n",
    "    frame_length, frame_step = frame_size * fs, frame_stride * fs\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "\n",
    "    # Calculate total number of frames and pad the signal if necessary\n",
    "    signal_length = len(signal)\n",
    "    total_frames = int(np.ceil(float(np.abs(signal_length - frame_length) / frame_step)))\n",
    "    padded_signal_length = total_frames * frame_step + frame_length\n",
    "\n",
    "    # Zero-padding the signal to match the required frame length\n",
    "    zeros = np.zeros((padded_signal_length - signal_length))\n",
    "    padded_signal = np.append(signal, zeros)\n",
    "\n",
    "    # Create indices for frames (each row corresponds to a frame)\n",
    "    indices = np.tile(np.arange(0, frame_length), (total_frames, 1)) + np.tile(np.arange(0, total_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "\n",
    "    # Extract frames from the padded signal\n",
    "    frames = padded_signal[indices.astype(np.int32, copy=False)]\n",
    "\n",
    "    # Apply a Hamming window to each frame\n",
    "    window = np.hamming(frame_length)\n",
    "    frames_windowed = frames * window\n",
    "\n",
    "    return frames, frames_windowed, frame_length, total_frames\n",
    "\n",
    "\n",
    "# Define frame size and stride in seconds\n",
    "frame_size = 0.025\n",
    "frame_stride = 0.01\n",
    "\n",
    "# Apply framing and windowing to the emphasized signal\n",
    "frames, frames_windowed, frame_length, total_frames = framing(emphasized_signal, frame_size, frame_stride, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Short-Time Fourier Transform (STFT)\n",
    "\n",
    "STFT is used to convert the windowed time-domain signal into the frequency domain. Each frame is transformed into its frequency components, producing a spectrogram that represents how frequencies evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(frames, NFFT):\n",
    "    \"\"\"\n",
    "    Perform Short-Time Fourier Transform on the input frames.\n",
    "\n",
    "    Args:\n",
    "        frames (numpy.ndarray): The input frames, each row is a frame.\n",
    "        NFFT (int): Number of FFT points, determines the frequency resolution.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The magnitude spectrum of each frame.\n",
    "    \"\"\"\n",
    "    # Compute the magnitude of the FFT for each frame\n",
    "    # rfft computes the one-dimensional n-point discrete Fourier Transform (DFT)\n",
    "    # and returns the non-negative frequency terms (real FFT).\n",
    "    mag_frames = np.abs(np.fft.rfft(frames, NFFT))\n",
    "\n",
    "    # Compute the power spectrum (squared magnitude normalized by the number of FFT points)\n",
    "    # Power spectrum gives the distribution of power into frequency components.\n",
    "    pow_frames = ((1.0 / NFFT) * (mag_frames ** 2))\n",
    "\n",
    "    return pow_frames\n",
    "\n",
    "# Set the number of FFT points (frequency resolution)\n",
    "NFFT = 512\n",
    "\n",
    "# Perform STFT on the frames\n",
    "spectrum = stft(frames, NFFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Mel-filter Bank\n",
    "\n",
    "The Mel-filter bank is applied to the STFT output to approximate human hearing perception. It compresses the frequency scale to focus more on lower frequencies, mimicking how humans perceive sound. This results in the Mel spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_filter_bank(num_filters, NFFT, fs):\n",
    "    \"\"\"\n",
    "    Generate Mel filter banks.\n",
    "\n",
    "    Args:\n",
    "        num_filters (int): The number of Mel filters.\n",
    "        NFFT (int): Number of FFT points, determines frequency resolution.\n",
    "        fs (int): Sampling rate of the signal.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Mel filter banks, shape (num_filters, NFFT // 2 + 1).\n",
    "    \"\"\"\n",
    "    # Convert the low and high frequencies to the Mel scale\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = 2595 * np.log10(1 + (fs / 2) / 700)\n",
    "\n",
    "    # Create evenly spaced Mel points\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, num_filters + 2)\n",
    "\n",
    "    # Convert Mel points back to Hz\n",
    "    hz_points = 700 * (10 ** (mel_points / 2595) - 1)\n",
    "\n",
    "    # Map Hz points to corresponding FFT bin numbers\n",
    "    bin_points = np.floor((NFFT + 1) * hz_points / fs)\n",
    "\n",
    "    # Initialize the filter bank matrix\n",
    "    filters = np.zeros((num_filters, int(np.floor(NFFT / 2 + 1))))\n",
    "\n",
    "    # Create triangular filters between successive Mel points\n",
    "    for m in range(1, num_filters + 1):\n",
    "        f_m_minus = int(bin_points[m - 1])\n",
    "        f_m = int(bin_points[m])\n",
    "        f_m_plus = int(bin_points[m + 1])\n",
    "\n",
    "        # Construct the left side of the triangular filter\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            filters[m - 1, k] = (k - bin_points[m - 1]) / (bin_points[m] - bin_points[m - 1])\n",
    "\n",
    "        # Construct the right side of the triangular filter\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            filters[m - 1, k] = (bin_points[m + 1] - k) / (bin_points[m + 1] - bin_points[m])\n",
    "\n",
    "    return filters\n",
    "\n",
    "# Set the number of Mel filters\n",
    "num_filters = 40\n",
    "\n",
    "# Generate the Mel filter bank and apply it to the spectrum\n",
    "filters = mel_filter_bank(num_filters, NFFT, fs)\n",
    "mel_spectrum = np.dot(spectrum, filters.T)\n",
    "\n",
    "# Replace zero values in the Mel spectrum with a small positive value to avoid log issues\n",
    "mel_spectrum = np.where(mel_spectrum == 0, np.finfo(float).eps, mel_spectrum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Log Transformation\n",
    "\n",
    "After applying the Mel-filter bank, we take the logarithm of the Mel spectrum to convert the amplitudes to a logarithmic scale, which aligns with how the human ear perceives sound intensity changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_magnitude(x):\n",
    "    \"\"\"\n",
    "    Apply logarithmic compression to the input spectrum to simulate human perception.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): The input spectrum (e.g., Mel spectrum).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The logarithmically compressed spectrum.\n",
    "    \"\"\"\n",
    "    # Convert to logarithmic scale (in dB)\n",
    "    return 10 * np.log10(x)\n",
    "\n",
    "# Apply log transformation to the Mel spectrum\n",
    "log_mel_spectrum = log_magnitude(mel_spectrum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Discrete Cosine Transform (DCT)\n",
    "\n",
    "DCT is applied to the log Mel spectrum to obtain the Mel Frequency Cepstral Coefficients (MFCCs). These coefficients represent the most important characteristics of the speech signal and are often used for tasks like speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import dct\n",
    "# Apply DCT to the log Mel spectrum to compute MFCC features\n",
    "mfcc_features = dct(log_mel_spectrum, type=2, axis=1, norm='ortho')[:, :13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Dynamic Feature Extraction\n",
    "\n",
    "In this step, the first-order (Delta) and second-order (Delta-Delta) derivatives of MFCCs are calculated. These dynamic features capture changes in the speech signal over time, providing additional temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(feature_matrix, N=2):\n",
    "    \"\"\"\n",
    "    Calculate delta (derivative) of the feature matrix.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix (numpy.ndarray): Input feature matrix (e.g., MFCCs).\n",
    "        N (int): The window size for calculating the delta.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Delta feature matrix.\n",
    "    \"\"\"\n",
    "    # Number of frames in the feature matrix\n",
    "    num_frames, _ = feature_matrix.shape\n",
    "\n",
    "    # Denominator for the delta calculation\n",
    "    denominator = 2 * sum([i ** 2 for i in range(1, N + 1)])\n",
    "\n",
    "    # Initialize the delta feature matrix with the same shape\n",
    "    delta_feature = np.empty_like(feature_matrix)\n",
    "\n",
    "    # Pad the feature matrix at the edges to handle boundary conditions\n",
    "    padded = np.pad(feature_matrix, ((N, N), (0, 0)), mode='edge')\n",
    "\n",
    "    # Compute the delta for each frame\n",
    "    for t in range(num_frames):\n",
    "        delta_feature[t] = np.dot(np.arange(-N, N + 1), padded[t: t + 2 * N + 1]) / denominator\n",
    "\n",
    "    return delta_feature\n",
    "\n",
    "# Compute the first-order delta (Delta) of the MFCC features\n",
    "delta1 = delta(mfcc_features)\n",
    "\n",
    "# Compute the second-order delta (Delta-Delta) of the first-order delta\n",
    "delta2 = delta(delta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Feature Transformation\n",
    "\n",
    "The MFCCs, along with their dynamic features (Delta and Delta-Delta), are normalized to ensure that each feature has a mean of zero and unit variance. This helps to remove biases and scale differences between features, improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the MFCC, Delta, and Delta-Delta features horizontally (combine them into one feature set)\n",
    "stacked_features = np.hstack((mfcc_features, delta1, delta2))\n",
    "\n",
    "# Mean normalization: subtract the mean of each feature across all frames\n",
    "cmn_features = stacked_features - np.mean(stacked_features, axis=0)\n",
    "\n",
    "# Variance normalization: divide by the standard deviation of each feature\n",
    "cvn_features = cmn_features / np.std(cmn_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is performed to reduce the dimensionality of the feature set while retaining the most important information. By keeping only the most significant principal components, this step helps to remove redundancy and improve computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "def feature_transformation(features, n_components=12):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the feature set.\n",
    "\n",
    "    Args:\n",
    "        features (numpy.ndarray): The input features to be transformed.\n",
    "        n_components (int): Number of principal components to keep.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Transformed features, PCA object)\n",
    "    \"\"\"\n",
    "    # Initialize PCA with the desired number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit the PCA model to the features and transform the data\n",
    "    transformed_features = pca.fit_transform(features)\n",
    "\n",
    "    return transformed_features, pca\n",
    "\n",
    "# Apply PCA to reduce the dimensionality of the stacked features (MFCC, Delta, Delta-Delta)\n",
    "transformed_features, pca_model = feature_transformation(stacked_features)\n",
    "\n",
    "# 保存特征值到 pkl 文件\n",
    "with open('features.pkl', 'wb') as f:\n",
    "    pickle.dump(transformed_features, f)\n",
    "    pickle.dump(pca_model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "source": [
    "# 11 - Comparison with `librosa` MFCC\n",
    "\n",
    "In this step, we compare the custom MFCC implementation with the MFCCs generated by the librosa library to validate our custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hop length based on the frame stride and sampling rate\n",
    "hop_length = int(frame_stride * fs)\n",
    "\n",
    "# Use librosa to compute MFCCs directly from the signal\n",
    "librosa_mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=13, n_fft=NFFT, hop_length=hop_length, n_mels=num_filters)\n",
    "\n",
    "# Ensure the custom MFCCs and librosa MFCCs have the same shape by trimming or padding as necessary\n",
    "if librosa_mfcc.shape[1] > mfcc_features.shape[0]:\n",
    "    # Trim the librosa MFCC to match the custom MFCC length\n",
    "    librosa_mfcc = librosa_mfcc[:, :mfcc_features.shape[0]]\n",
    "elif librosa_mfcc.shape[1] < mfcc_features.shape[0]:\n",
    "    # Trim the custom MFCC to match the librosa MFCC length\n",
    "    mfcc_features = mfcc_features[:librosa_mfcc.shape[1], :]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfcc_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
